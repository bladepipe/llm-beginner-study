# 🔑 新手三问（精简版）

1. 什么是 Bigram 模型？  
P('e'|'h') —— 仅凭前一个字符预测下一个。如 `'h'→'e'` 在训练中占 60%，模型即学得 0.6。

2. 它如何“学习”？  
① 扫文本；② 统计 `'h'→'e'` 等频次；③ 归一化为概率（例：`'h'` 出现 20 次，`'e'` 跟 12 次 → 0.6）。

3. 如何生成文本？  
给 `'t'` → 查表得 `'h':0.62`, `'e':0.12` → 加权随机选 `'h'` → 再查 `'h'` 行 → 循环。  
→ 这就是 GPT 的自回归生成（autoregressive generation）。