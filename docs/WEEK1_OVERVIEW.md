# ğŸ“š WEEK 1: Bigram Foundations

## ğŸ§­ Learning Path (Text Diagram)

```
Shakespeare text
     â†“
Count pairs ('h'â†’'e')
     â†“
Normalize â†’ P('e'|'h') = 0.6
     â†“
Sample â†’ 'h' â†’ 'e' â†’ 'l' â†’ ...
```

## ğŸ—‚ï¸ Key Concept Cards

### 1. Bigram Model
- **EN**: Predicts next char *only* from previous one: `P(w_t | w_{t-1})`
- **ä¸­æ–‡**: ä»…å‡­å‰ä¸€ä¸ªå­—ç¬¦é¢„æµ‹ä¸‹ä¸€ä¸ªï¼Œå¦‚ `P('e'|'h')`

### 2. Counting Approach
- **EN**: No gradients. Just count & divide.
- **ä¸­æ–‡**: æ— éœ€æ¢¯åº¦ã€‚åªç»Ÿè®¡ã€å†å½’ä¸€åŒ–ã€‚

### 3. Autoregressive Generation
- **EN**: Start â†’ lookup â†’ sample â†’ repeat.
- **ä¸­æ–‡**: èµ·å§‹ â†’ æŸ¥è¡¨ â†’ é‡‡æ · â†’ å¾ªç¯ã€‚

---
âœ… Done. Next: Add `bigram_scratch.py`.