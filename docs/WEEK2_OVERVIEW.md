# WEEK 2: Tokenization & Embedding
> ğŸ’¡ å°†å­—ç¬¦åºåˆ—è½¬åŒ–ä¸ºå‘é‡â€”â€”Bigram åˆ° GPT çš„å…³é”®è·ƒè¿ã€‚

---

| English | ä¸­æ–‡ |
|---------|------|
| `encode()` / `decode()` | æŠŠæ–‡æœ¬è½¬ä¸ºæ•´æ•°åºåˆ—ï¼Œå†è¿˜åŸå›æ¥ |
| `nn.Embedding(vocab_size, n_embd)` | æŠŠæ¯ä¸ªæ•´æ•°æ˜ å°„æˆä¸€ä¸ªç¨ å¯†å‘é‡ |
| `shakespeare.txt` â†’ `[1,27,3,...]` â†’ `[[0.2,-0.8,...], ...]` | æ•°æ®æµæ¸…æ™°å¯è§ |

âœ… ä¸ºä»€ä¹ˆæ˜¯ WEEK 2ï¼Ÿ
â€¢ å®ƒè®©æ¨¡å‹â€œçœ‹è§â€è¯­ä¹‰ï¼Œè€Œä¸ä»…æ˜¯ç»Ÿè®¡å…±ç°ï¼›
â€¢ å®ƒæ˜¯ `minGPT-light/model.py` ä¸­ `self.token_embedding_table` çš„ç‹¬ç«‹å®ç°ã€‚

â–¶ï¸ ç«‹å³å®è·µï¼š
```python
from projects.minGPT-light.tokenizer import Tokenizer
from projects.minGPT-light.embedding import Embedding

t = Tokenizer()
e = Embedding(vocab_size=65, n_embd=32)

ids = t.encode("hello world")
vecs = e(torch.tensor(ids))
print(vecs.shape)  # (11, 32)
```
